# module for configuring the RL agent
# configuration parameters are loaded and used both when using the agent via the CLI and via the interface
# all parameters are required, defaults are in comments

# observation_space = ['ingress_traffic', 'node_load']
observation_space:
  - ingress_traffic

# Setting episode steps
episode_steps: 96

# Agent type: SAC or DDPG, DDPG_Baseline, TD3_Baseline, 
agent_type: "DDPG_Baseline"

# # Policy to be used. Available: SPRPolicy, MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy, TD3Policy, ContinuousCritic, CnnPolicy
policy: TD3Policy

# Policy and Value function NN sizes
layers: [64]

# objective and reward
objective: 'sfc_with_priority'
flow_weight: 0.5
delay_weight: 0.5

# for noise random process
rand_sigma: 0.2 # 0.2 - 0.3 seem to work well here

# for discounted return, when calculating the Q values and training the critic
gamma: 0.99 # default 0.99

# learning rate alpha: size of update steps during learning (too low -> slow convergence, too high -> divergence)
learning_rate: 0.01 # default 0.001

buffer_size: 10000

batch_size: 64

tau: 0.0001

gradient_steps: -1

optimize_memory_usage: False

create_eval_env: False

verbose: 1

learning_starts: 96